{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Multi-Task Learning Model for Battery Health Prediction\n",
    "\n",
    "This notebook implements a multi-task learning approach for battery health prediction,\n",
    "including State of Health (SOH), Remaining Useful Life (RUL), and Discharge Capacity Change Rate (DCCR) \n",
    "prediction using gradient reversal and auxiliary networks.\n",
    "\n",
    "The code is for hyperparameter optimization based on Bayesian optimization.\n",
    "\n",
    "Author: Rong ZHU, Weiwen Peng*\n",
    "\n",
    "Date: 2025/07/16"
   ],
   "id": "37140044010da3dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Set device for computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "3095cbfa"
  },
  {
   "cell_type": "code",
   "id": "f0ca6cae",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Data Loading and Preprocessing\n",
    "# =============================================================================\n",
    "\n",
    "# Define batch names for battery data\n",
    "batch_list = ['batch01', 'batch02', 'batch03', 'batch04', 'batch05', 'batch06', 'batch07',\n",
    "              'batch08', 'batch09', 'batch10', 'batch11', 'batch12', 'batch13', 'batch14',\n",
    "              'batch15', 'batch16', 'batch17', 'batch18', 'batch19', 'batch20', 'batch21']\n",
    "\n",
    "# Load battery data from MATLAB files\n",
    "CM_data = scipy.io.loadmat('CM_Data1126.mat')\n",
    "CM_data_aux = scipy.io.loadmat('CM_data_aux_label.mat')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d3026104",
   "metadata": {},
   "source": [
    "# Initialize arrays for training data\n",
    "train_feature_all = np.array([])\n",
    "train_SOH_label_all = np.array([])\n",
    "train_RUL_label_all = np.array([])\n",
    "train_RE_label_all = np.array([])\n",
    "\n",
    "# Extract features and labels from all batches\n",
    "# Note: extract_feature function needs to be defined separately\n",
    "for batch_num in range(21):\n",
    "    for battery_num in range(4):\n",
    "        # Extract features and labels for current batch and battery\n",
    "        feature, SOH_label, RUL_label, RE_label = extract_feature(batch_num, battery_num)\n",
    "        \n",
    "        # Skip the first battery (battery_num == 0) for some reason\n",
    "        if battery_num != 0:\n",
    "            try:\n",
    "                # Concatenate features and labels\n",
    "                train_feature_all = np.concatenate((train_feature_all, feature), axis=0)\n",
    "                train_SOH_label_all = np.concatenate((train_SOH_label_all, SOH_label), axis=0)\n",
    "                # Note: There's a typo in original code - \"train_RUL_labe_alll\" should be \"train_RUL_label_all\"\n",
    "                train_RUL_label_all = np.concatenate((train_RUL_label_all, RUL_label), axis=0)\n",
    "                train_RE_label_all = np.concatenate((train_RE_label_all, RE_label), axis=0)\n",
    "            except:\n",
    "                # Initialize arrays if this is the first valid data\n",
    "                train_feature_all = feature\n",
    "                train_SOH_label_all = SOH_label\n",
    "                train_RUL_label_all = RUL_label\n",
    "                train_RE_label_all = RE_label\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "974e5590",
   "metadata": {},
   "source": [
    "# Normalize features using min-max normalization\n",
    "max_train_feature = np.max(train_feature_all, axis=0)\n",
    "min_train_feature = np.min(train_feature_all, axis=0)\n",
    "train_feature_all = (train_feature_all - min_train_feature) / (max_train_feature - min_train_feature)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_feature, val_feature, train_SOH_label, val_SOH_label = train_test_split(\n",
    "    train_feature_all, train_SOH_label_all, test_size=0.15, random_state=42)\n",
    "\n",
    "# Split remaining labels (note: inconsistent splitting - should use same indices)\n",
    "train_RUL_label, val_RUL_label = train_test_split(train_RUL_label_all, test_size=0.15, random_state=42)\n",
    "train_RE_label, val_RE_label = train_test_split(train_RE_label_all, test_size=0.15, random_state=42)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "train_x = torch.from_numpy(train_feature).float()\n",
    "train_SOH = torch.from_numpy(train_SOH_label).float()\n",
    "train_RUL = torch.from_numpy(train_RUL_label).float()\n",
    "train_RE = torch.from_numpy(train_RE_label).float()\n",
    "\n",
    "val_x = torch.from_numpy(val_feature).float()\n",
    "val_SOH = torch.from_numpy(val_SOH_label).float()\n",
    "val_RUL = torch.from_numpy(val_RUL_label).float()\n",
    "val_RE = torch.from_numpy(val_RE_label).float()\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TensorDataset(train_x, train_SOH, train_RUL, train_RE)\n",
    "val_dataset = TensorDataset(val_x, val_SOH, val_RUL, val_RE)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=128, shuffle=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8dac6ec4",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Model Architecture\n",
    "# =============================================================================\n",
    "\n",
    "class GradReverse(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer for Domain Adaptation\n",
    "    \n",
    "    This layer acts as an identity function during forward pass but reverses\n",
    "    the gradient during backward pass, multiplied by a lambda parameter.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambd=1.0):\n",
    "        ctx.lambd = lambd\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg().mul(ctx.lambd), None\n",
    "\n",
    "def grad_reverse(x, lambd=1.0):\n",
    "    \"\"\"Convenience function for gradient reversal\"\"\"\n",
    "    return GradReverse.apply(x, lambd)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron with configurable architecture\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Input feature dimension\n",
    "        output_dim: Output dimension\n",
    "        layers_num: Number of layers (must be >= 2)\n",
    "        hidden_dim: Hidden layer dimension\n",
    "        dropout: Dropout probability\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=59, output_dim=1, layers_num=4, hidden_dim=50, dropout=0.01):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        assert layers_num >= 2, \"Number of layers must be greater than or equal to 2\"\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layers_num = layers_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Build network layers\n",
    "        self.layers = []\n",
    "        for i in range(layers_num):\n",
    "            if i == 0:\n",
    "                # Input layer\n",
    "                self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            elif i == layers_num - 1:\n",
    "                # Output layer\n",
    "                self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "            else:\n",
    "                # Hidden layers\n",
    "                self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                self.layers.append(nn.ReLU())\n",
    "                self.layers.append(nn.Dropout(p=dropout))\n",
    "        \n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier normal initialization\"\"\"\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Solution_u(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Task Learning Model for Battery Health Prediction\n",
    "    \n",
    "    This model consists of:\n",
    "    - A shared feature extractor (encoder)\n",
    "    - Three task-specific predictors for SOH, RUL, and RE\n",
    "    \n",
    "    Args:\n",
    "        feature_extractor_layer: Number of layers in feature extractor\n",
    "        feature_extractor_hidden_num: Hidden units in feature extractor\n",
    "        SOH_predictor_layer: Number of layers in SOH predictor\n",
    "        SOH_predictor_hidden_num: Hidden units in SOH predictor\n",
    "        RUL_predictor_layer: Number of layers in RUL predictor\n",
    "        RUL_predictor_hidden_num: Hidden units in RUL predictor\n",
    "        RE_predictor_layer: Number of layers in RE predictor\n",
    "        RE_predictor_hidden_num: Hidden units in RE predictor\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 feature_extractor_layer=3,\n",
    "                 feature_extractor_hidden_num=60,\n",
    "                 SOH_predictor_layer=1,\n",
    "                 SOH_predictor_hidden_num=60,\n",
    "                 RUL_predictor_layer=1,\n",
    "                 RUL_predictor_hidden_num=60,\n",
    "                 RE_predictor_layer=1,\n",
    "                 RE_predictor_hidden_num=60):\n",
    "        super(Solution_u, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        # Note: Need to fix feature.shape[1] - should be passed as parameter\n",
    "        self.encoder = MLP(\n",
    "            input_dim=59,  # This should be feature.shape[1]\n",
    "            output_dim=32,\n",
    "            layers_num=feature_extractor_layer,\n",
    "            hidden_dim=feature_extractor_hidden_num,\n",
    "            dropout=0.01\n",
    "        )\n",
    "        \n",
    "        # Task-specific predictors\n",
    "        self.SOH_predictor = MLP(\n",
    "            input_dim=32,\n",
    "            output_dim=1,\n",
    "            layers_num=SOH_predictor_layer,\n",
    "            hidden_dim=SOH_predictor_hidden_num,\n",
    "            dropout=0.01\n",
    "        )\n",
    "        \n",
    "        self.RUL_predictor = MLP(\n",
    "            input_dim=32,\n",
    "            output_dim=1,\n",
    "            layers_num=RUL_predictor_layer,\n",
    "            hidden_dim=RUL_predictor_hidden_num,\n",
    "            dropout=0.01\n",
    "        )\n",
    "        \n",
    "        self.RE_predictor = MLP(\n",
    "            input_dim=32,\n",
    "            output_dim=1,\n",
    "            layers_num=RE_predictor_layer,\n",
    "            hidden_dim=RE_predictor_hidden_num,\n",
    "            dropout=0.01\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the multi-task model\n",
    "        \n",
    "        Args:\n",
    "            x: Input features\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (SOH_prediction, RUL_prediction, RE_prediction)\n",
    "        \"\"\"\n",
    "        # Extract shared features\n",
    "        features = self.encoder(x)\n",
    "        \n",
    "        # Make task-specific predictions\n",
    "        SOH = self.SOH_predictor(features)\n",
    "        RUL = self.RUL_predictor(features)\n",
    "        RE = self.RE_predictor(features)\n",
    "        \n",
    "        return SOH, RUL, RE\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize all network weights\"\"\"\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "            elif isinstance(layer, nn.Conv1d):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Mean Absolute Percentage Error Loss\n",
    "    \n",
    "    MAPE = mean(|y_true - y_pred| / |y_true|)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Add small epsilon to avoid division by zero\n",
    "        epsilon = 1e-8\n",
    "        loss = torch.abs((y_true - y_pred) / (y_true + epsilon))\n",
    "        return torch.mean(loss)\n",
    "\n",
    "\n",
    "class AuxiliaryNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Auxiliary Network for Gradient-based Task Identification\n",
    "    \n",
    "    This network takes gradients from different decoders and tries to\n",
    "    identify which decoder the gradient came from. Used for regularization\n",
    "    in multi-task learning.\n",
    "    \n",
    "    Args:\n",
    "        grad_dim: Dimension of gradient vectors\n",
    "        num_decoders: Number of decoders (tasks)\n",
    "    \"\"\"\n",
    "    def __init__(self, grad_dim, num_decoders):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(grad_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_decoders)\n",
    "        )\n",
    "    \n",
    "    def forward(self, grad_vec):\n",
    "        return self.net(grad_vec)\n",
    "\n",
    "\n",
    "def get_decoder_flatdim(decoder):\n",
    "    \"\"\"\n",
    "    Calculate the total number of parameters in a decoder\n",
    "    \n",
    "    Args:\n",
    "        decoder: PyTorch module\n",
    "        \n",
    "    Returns:\n",
    "        int: Total number of parameters\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    for p in decoder.parameters():\n",
    "        total_params += p.numel()\n",
    "    return total_params\n",
    "\n",
    "\n",
    "def accuracy_cal(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate various accuracy metrics\n",
    "    \n",
    "    Args:\n",
    "        y_pred: Predicted values\n",
    "        y_true: True values\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (RMSE, MAE, MAPE, R2)\n",
    "    \"\"\"\n",
    "    # This function needs to be implemented\n",
    "    # Placeholder implementation\n",
    "    mse = torch.mean((y_pred - y_true) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    mae = torch.mean(torch.abs(y_pred - y_true))\n",
    "    \n",
    "    # MAPE calculation\n",
    "    epsilon = 1e-8\n",
    "    mape = torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon)))\n",
    "    \n",
    "    # R2 calculation\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    \n",
    "    return rmse.item(), mae.item(), mape.item(), r2.item()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35296dc1",
   "metadata": {},
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Hyperparameter Optimization with Optuna\n",
    "# =============================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial object\n",
    "        \n",
    "    Returns:\n",
    "        float: Objective value to maximize (average R2 score)\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters\n",
    "    feature_extractor_layer = trial.suggest_int('feature_extractor_layer', 3, 10)\n",
    "    feature_extractor_hidden_num = trial.suggest_int('feature_extractor_hidden_num', 10, 100)\n",
    "    \n",
    "    SOH_predictor_layer = trial.suggest_int('SOH_predictor_layer', 2, 5)\n",
    "    SOH_predictor_hidden_num = trial.suggest_int('SOH_predictor_hidden_num', 10, 100)\n",
    "    \n",
    "    RUL_predictor_layer = trial.suggest_int('RUL_predictor_layer', 2, 5)\n",
    "    RUL_predictor_hidden_num = trial.suggest_int('RUL_predictor_hidden_num', 10, 100)\n",
    "    \n",
    "    RE_predictor_layer = trial.suggest_int('RE_predictor_layer', 2, 5)\n",
    "    RE_predictor_hidden_num = trial.suggest_int('RE_predictor_hidden_num', 10, 100)\n",
    "    \n",
    "    epochs = trial.suggest_int('epochs', 100, 1800)\n",
    "    miu = trial.suggest_float('miu', 1e-3, 1, log=True)\n",
    "    gamma = trial.suggest_float('gamma', 1e-3, 1, log=True)\n",
    "    lambda_aux = trial.suggest_float('lambda_aux', 1e-3, 1, log=True)\n",
    "    \n",
    "    # Create model with suggested hyperparameters\n",
    "    model = Solution_u(\n",
    "        feature_extractor_layer=feature_extractor_layer,\n",
    "        feature_extractor_hidden_num=feature_extractor_hidden_num,\n",
    "        SOH_predictor_layer=SOH_predictor_layer,\n",
    "        SOH_predictor_hidden_num=SOH_predictor_hidden_num,\n",
    "        RUL_predictor_layer=RUL_predictor_layer,\n",
    "        RUL_predictor_hidden_num=RUL_predictor_hidden_num,\n",
    "        RE_predictor_layer=RE_predictor_layer,\n",
    "        RE_predictor_hidden_num=RE_predictor_hidden_num\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    # Calculate dimensions for auxiliary network\n",
    "    decoder_flat_dims = [\n",
    "        get_decoder_flatdim(model.SOH_predictor),\n",
    "        get_decoder_flatdim(model.RUL_predictor),\n",
    "        get_decoder_flatdim(model.RE_predictor)\n",
    "    ]\n",
    "    max_dim = max(decoder_flat_dims)\n",
    "    \n",
    "    # Create auxiliary network\n",
    "    auxnet = AuxiliaryNet(max_dim, 3)\n",
    "    auxnet.to(device)\n",
    "    \n",
    "    # Define loss functions and optimizers\n",
    "    lr = 0.001\n",
    "    loss_data = MAPELoss()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    aux_criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Separate optimizers for different components\n",
    "    optimizer_shared = torch.optim.Adam(model.encoder.parameters(), lr=lr)\n",
    "    optimizer_SOH = torch.optim.Adam(model.SOH_predictor.parameters(), lr=lr)\n",
    "    optimizer_RUL = torch.optim.Adam(model.RUL_predictor.parameters(), lr=lr)\n",
    "    optimizer_RE = torch.optim.Adam(model.RE_predictor.parameters(), lr=lr)\n",
    "    optimizer_aux = torch.optim.Adam(auxnet.parameters(), lr=lr)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (x, y, rul, re) in enumerate(train_loader):\n",
    "            x, y, rul, re = x.to(device), y.to(device), rul.to(device), re.to(device)\n",
    "            \n",
    "            # Clear gradients\n",
    "            optimizer_shared.zero_grad()\n",
    "            optimizer_SOH.zero_grad()\n",
    "            optimizer_RUL.zero_grad()\n",
    "            optimizer_RE.zero_grad()\n",
    "            optimizer_aux.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            u, RUL, RE = model(x)\n",
    "            \n",
    "            # Calculate task-specific losses\n",
    "            loss_soh = loss_data(u, y)\n",
    "            loss_rul = miu * loss_data(RUL, rul)\n",
    "            loss_re = gamma * loss_fn(RE, re)\n",
    "            \n",
    "            # Backward pass for SOH predictor\n",
    "            loss_soh.backward(retain_graph=True)\n",
    "            grad_soh_list = []\n",
    "            for p in model.SOH_predictor.parameters():\n",
    "                if p.grad is not None:\n",
    "                    grad_soh_list.append(p.grad.detach().view(-1))\n",
    "            optimizer_SOH.zero_grad()\n",
    "            \n",
    "            # Backward pass for RUL predictor\n",
    "            loss_rul.backward(retain_graph=True)\n",
    "            grad_rul_list = []\n",
    "            for p in model.RUL_predictor.parameters():\n",
    "                if p.grad is not None:\n",
    "                    grad_rul_list.append(p.grad.detach().view(-1))\n",
    "            optimizer_RUL.zero_grad()\n",
    "            \n",
    "            # Backward pass for RE predictor\n",
    "            loss_re.backward(retain_graph=True)\n",
    "            grad_re_list = []\n",
    "            for p in model.RE_predictor.parameters():\n",
    "                if p.grad is not None:\n",
    "                    grad_re_list.append(p.grad.detach().view(-1))\n",
    "            optimizer_RE.zero_grad()\n",
    "            \n",
    "            # Update main model parameters\n",
    "            optimizer_shared.step()\n",
    "            optimizer_SOH.step()\n",
    "            optimizer_RUL.step()\n",
    "            optimizer_RE.step()\n",
    "            \n",
    "            # Clear gradients for auxiliary network training\n",
    "            optimizer_shared.zero_grad()\n",
    "            optimizer_SOH.zero_grad()\n",
    "            optimizer_RUL.zero_grad()\n",
    "            optimizer_RE.zero_grad()\n",
    "            optimizer_aux.zero_grad()\n",
    "            \n",
    "            # Helper function to pad gradients to same dimension\n",
    "            def pad_zero(vec, target_dim):\n",
    "                cur_dim = vec.shape[1]\n",
    "                if cur_dim < target_dim:\n",
    "                    pad_size = target_dim - cur_dim\n",
    "                    zero_pad = torch.zeros(vec.shape[0], pad_size, device=vec.device)\n",
    "                    return torch.cat([vec, zero_pad], dim=1)\n",
    "                else:\n",
    "                    return vec\n",
    "            \n",
    "            # Process gradients for auxiliary network\n",
    "            def process_decoder_grad(grad_list, decoder_id):\n",
    "                if len(grad_list) == 0:\n",
    "                    return None\n",
    "                \n",
    "                # Concatenate gradients and pad to max dimension\n",
    "                grad_vec = torch.cat(grad_list, dim=0).unsqueeze(0).to(device)\n",
    "                grad_vec = pad_zero(grad_vec, max_dim)\n",
    "                \n",
    "                # Apply gradient reversal\n",
    "                rev_feat = grad_reverse(grad_vec, lambda_aux)\n",
    "                \n",
    "                # Forward through auxiliary network\n",
    "                aux_out = auxnet(rev_feat)\n",
    "                label_t = torch.tensor([decoder_id], dtype=torch.long, device=device)\n",
    "                aux_loss_i = aux_criterion(aux_out, label_t)\n",
    "                aux_loss_i.backward()\n",
    "                \n",
    "                return aux_loss_i.item()\n",
    "            \n",
    "            # Process gradients from each decoder\n",
    "            process_decoder_grad(grad_soh_list, 0)  # SOH decoder\n",
    "            process_decoder_grad(grad_rul_list, 1)  # RUL decoder\n",
    "            process_decoder_grad(grad_re_list, 2)   # RE decoder\n",
    "            \n",
    "            # Update auxiliary network\n",
    "            optimizer_aux.step()\n",
    "    \n",
    "    # Evaluate model on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_soh_pred, val_rul_pred, _ = model(val_x.to(device))\n",
    "        \n",
    "        # Calculate metrics for SOH and RUL\n",
    "        val_rmse, val_mae, val_mape, val_r2 = accuracy_cal(val_soh_pred, val_SOH.to(device))\n",
    "        val_rmse1, val_mae1, val_mape1, val_r21 = accuracy_cal(val_rul_pred, val_RUL.to(device))\n",
    "    \n",
    "    # Return average R2 score as objective\n",
    "    return (val_r2 + val_r21) / 2\n",
    "\n",
    "\n",
    "def print_progress(study, trial):\n",
    "    \"\"\"Callback function to print optimization progress\"\"\"\n",
    "    result = (f\"Trial {trial.number} finished with value: {trial.value:.4f} \"\n",
    "              f\"and parameters: {trial.params}. \"\n",
    "              f\"Best is trial {study.best_trial.number} with value: {study.best_value:.4f}.\")\n",
    "    print(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0a8d53b",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Main Optimization Loop\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create Optuna study and optimize\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=200, callbacks=[print_progress])\n",
    "    \n",
    "    # Print best hyperparameters\n",
    "    print('\\nBest trial:')\n",
    "    trial = study.best_trial\n",
    "    print(f'  Value: {trial.value:.4f}')\n",
    "    print('  Params: ')\n",
    "    for key, value in trial.params.items():\n",
    "        print(f'    {key}: {value}')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
